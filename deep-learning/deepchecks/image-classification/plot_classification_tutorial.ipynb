{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Classification Tutorial \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ivanr\\appdata\\roaming\\python\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ivanr\\appdata\\roaming\\python\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ivanr\\appdata\\roaming\\python\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ivanr\\appdata\\roaming\\python\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ivanr\\appdata\\roaming\\python\\python38\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -illow (c:\\users\\ivanr\\appdata\\roaming\\python\\python38\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install \"deepchecks[vision]\" --quiet --upgrade"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data\n",
        "=========\n",
        "\n",
        "We will use torchvision and torch.utils.data packages for loading the\n",
        "data. The model we are building will learn to classify **ants** and\n",
        "**bees**. We have about 120 training images each for ants and bees.\n",
        "There are 75 validation images for each class. This dataset is a very\n",
        "small subset of imagenet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL.Image\n",
        "import torch\n",
        "import torchvision\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class CustomDataset(torchvision.datasets.ImageFolder):\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        \"\"\"overrides __getitem__ to be compatible to albumentations\"\"\"\n",
        "        path, target = self.samples[index]\n",
        "        sample = self.loader(path)\n",
        "        sample = self.get_cv2_image(sample)\n",
        "        if self.transforms is not None:\n",
        "            transformed = self.transforms(image=sample, target=target)\n",
        "            sample, target = transformed[\"image\"], transformed[\"target\"]\n",
        "        else:\n",
        "            if self.transform is not None:\n",
        "                sample = self.transform(image=sample)['image']\n",
        "            if self.target_transform is not None:\n",
        "                target = self.target_transform(target)\n",
        "\n",
        "        return sample, target\n",
        "\n",
        "    def get_cv2_image(self, image):\n",
        "        if isinstance(image, PIL.Image.Image):\n",
        "            return np.array(image).astype('uint8')\n",
        "        elif isinstance(image, np.ndarray):\n",
        "            return image\n",
        "        else:\n",
        "            raise RuntimeError(\"Only PIL.Image and CV2 loaders currently supported!\")\n",
        "\n",
        "data_dir = '../data/SUR'\n",
        "# Just normalization for validation\n",
        "data_transforms = A.Compose([\n",
        "    A.Resize(height=256, width=256),\n",
        "    A.CenterCrop(height=224, width=224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "train_dataset = CustomDataset(root=os.path.join(data_dir,'train'))\n",
        "train_dataset.transforms = data_transforms\n",
        "\n",
        "test_dataset = CustomDataset(root=os.path.join(data_dir, 'test'))\n",
        "test_dataset.transforms = data_transforms"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the dataset\n",
        "=====================\n",
        "\n",
        "Let\\'s see how our data looks like.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training images: 4800\n",
            "Number of validation images: 1200\n",
            "Example output of an image shape: torch.Size([3, 224, 224])\n",
            "Example output of a label: 0\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of training images: {len(train_dataset)}')\n",
        "print(f'Number of validation images: {len(test_dataset)}')\n",
        "print(f'Example output of an image shape: {train_dataset[0][0].shape}')\n",
        "print(f'Example output of a label: {train_dataset[0][1]}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading a pre-trained model\n",
        "===============================\n",
        "\n",
        "Now, we will download a pre-trained model from torchvision, that was\n",
        "trained on the ImageNet dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "# We have only 2 classes\n",
        "model.fc = nn.Linear(num_ftrs, 6)\n",
        "model = model.to(device)\n",
        "_ = model.eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Validating the Model with Deepchecks\n",
        "\n",
        "Now, after we have the training data, validation data and the model, we\n",
        "can validate the model with deepchecks test suites.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.vision_data import BatchOutputFormat\n",
        "\n",
        "def deepchecks_collate_fn(batch) -> BatchOutputFormat:\n",
        "    \"\"\"Return a batch of images, labels and predictions for a batch of data. The expected format is a dictionary with\n",
        "    the following keys: 'images', 'labels' and 'predictions', each value is in the deepchecks format for the task.\n",
        "    You can also use the BatchOutputFormat class to create the output.\n",
        "    \"\"\"\n",
        "    # batch received as iterable of tuples of (image, label) and transformed to tuple of iterables of images and labels:\n",
        "    batch = tuple(zip(*batch))\n",
        "\n",
        "    # images:\n",
        "    inp = torch.stack(batch[0]).detach().numpy().transpose((0, 2, 3, 1))\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "    inp = std * inp + mean\n",
        "    images = np.clip(inp, 0, 1) * 255\n",
        "\n",
        "    #labels:\n",
        "    labels = batch[1]\n",
        "\n",
        "    #predictions:\n",
        "    logits = model.to(device)(torch.stack(batch[0]).to(device))\n",
        "    predictions = nn.Softmax(dim=1)(logits)\n",
        "    return BatchOutputFormat(images=images, labels=labels, predictions=predictions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have a single label here, which is the tomato class The label\\_map is\n",
        "a dictionary that maps the class id to the class name, for display\n",
        "purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "LABEL_MAP = {\n",
        "    0: 'Ia',\n",
        "    1: 'IIa',\n",
        "    2: 'IIIa',\n",
        "    3: 'IVc',\n",
        "    4: 'IVd',\n",
        "    5: 'Va'\n",
        "  }"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our updated collate function, we can recreate the\n",
        "dataloader in the deepchecks format, and use it to create a VisionData\n",
        "object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision import VisionData\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=deepchecks_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=deepchecks_collate_fn)\n",
        "\n",
        "training_data = VisionData(batch_loader=train_loader, task_type='classification', label_map=LABEL_MAP)\n",
        "test_data = VisionData(batch_loader=test_loader, task_type='classification', label_map=LABEL_MAP)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Making sure our data is in the correct format:\n",
        "==============================================\n",
        "\n",
        "The VisionData object automatically validates your data format and will\n",
        "alert you if there is a problem. However, you can also manually view\n",
        "your images and labels to make sure they are in the correct format by\n",
        "using the `head` function to conveniently visualize your data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee3c2430007a49e983714382c11b977c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<div style=\"display:flex; flex-direction: column; gap: 10px;\">\\n                <di…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And observe the output:\n",
        "\n",
        "Running Deepchecks\\' suite on our data and model!\n",
        "=================================================\n",
        "\n",
        "Now that we have defined the task class, we can validate the train and\n",
        "test data with deepchecks\\' train test validation suite. This can be\n",
        "done with this simple few lines of code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.suites import train_test_validation\n",
        "\n",
        "suite = train_test_validation()\n",
        "result = suite.run(training_data, test_data,  max_samples = 5000)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also have suites for:\n",
        "`data integrity <deepchecks.vision.suites.data_integrity>`{.interpreted-text\n",
        "role=\"func\"} - validating a single dataset and\n",
        "`model evaluation <deepchecks.vision.suites.model_evaluation>`{.interpreted-text\n",
        "role=\"func\"} -evaluating the model\\'s performance.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observing the results:\n",
        "======================\n",
        "\n",
        "The results can be saved as a html file with the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'output_img_classification.html'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.save_as_html('output_img_classification.html')\n",
        "\n",
        "# Or displayed in a new window in an IDE\n",
        "# result.show_in_window()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or, if working inside a notebook, the output can be displayed directly\n",
        "by simply printing the result object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f48f6fc23c2433783f8643efabb4dcf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_8HZFFGHF3OBS0SWV5QD8C1AM0\">Train Test Validat…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "result"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
